<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SPA-VL</title>
  <link rel="icon" type="image/x-icon" href="static/images/8.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/8.jpg" alt="Icon" style="width: 40px; height: 40px">
              SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=rsbQ8twAAAAJ&hl=en&oi=ao" target="_blank">Yongting
                  Zhang</a><sup>1,3*</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Lu_Chen15" target="_blank">Lu Chen</a><sup>2,3*</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Guodong_Zheng1" target="_blank">Guodong
                  Zheng</a><sup>2,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Yifeng_Gao4" target="_blank">Yifeng Gao</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7Z0V_SoAAAAJ&hl=en&oi=ao" target="_blank">Rui
                  Zheng</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=D4vtw8QAAAAJ" target="_blank">Jinlan
                  Fu</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=ngPR1dIAAAAJ&hl=zh-CN" target="_blank">Zhenfei
                  Yin</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Senjie_Jin1" target="_blank">Senjie Jin</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN&oi=ao" target="_blank">Yu
                  Qiao</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=RGsMgZA4H78C" target="_blank">Xuanjing
                  Huang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=r6CvuOUAAAAJ" target="_blank">Feng
                  Zhao</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=BrOLQdwAAAAJ" target="_blank">Tao
                  Gui</a><sup>2,3â€ </sup>,
              </span>
              <span class="author-block">
                <a href="https://amandajshao.github.io/" target="_blank">Jing Shao</a><sup>3â€ </sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>University of Science and Technology of China,<br>
                <sup>2</sup>Fudan University,<br>
                <sup>3</sup>Shanghai Artificial Intelligence Laboratory
              </span>
              <span class="eql-cntrb">
                <small><br><sup>*</sup>Indicates Equal Contribution. Authorship order determined by coin
                  clip.</small><br>
                <small><sup>&dagger;</sup>Indicates Corresponding Authors.</small>
              </span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2406.12030" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Dataset link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/sqrti/SPA-VL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>

                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/EchoseChen/SPA-VL-RLHF" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Checkpoints Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/superjelly" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>ðŸ¤—Checkpoints</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/1.png" alt="Teaser Image" height="100%">
        <h2 class="subtitle has-text-centered">
          Overview of SPA-VL Dataset. It is built in three stages: 1) Image Collection, 2) Questions
          Constrution and 3) Preference Construction. The dataset examples shows vision-question-preferences
          pairs that comprise three types of questions: easy questions, hard questions, and hard statements.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->

  <!-- Result Image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/2.png" alt="Training Result Image" height="100%">
        <h2 class="subtitle has-text-centered">
          Comparison of different open source models and various dataset trained models on harmlessness. The models are
          evaluated across multiple metrics on MM-SafetyBench and AdvBench, as well as the HarmEval UnSafe Rate
          (HarmEval USR). After training on our proposed dataset, SPA-VL, the model achieves the best scores according
          all metric on both dpo and ppo methods.

        </h2>
      </div>
    </div>
  </section>
  <!-- End result image -->


  <!-- Paper abstract -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The emergence of Vision Language Models (VLMs) has brought unprecedented advances in understanding
              multimodal information.
              The combination of textual and visual semantics in VLMs is highly complex and diverse, making the safety
              alignment of these models challenging.
              Furthermore, due to the limited study on the safety alignment of VLMs, there is a lack of large-scale,
              high-quality datasets.
              To address these limitations, we propose a <strong>S</strong>afety <strong>P</strong>reference
              <strong>A</strong>lignment dataset
              for <strong>V</strong>ision <strong>L</strong>anguage Models named SPA-VL.
              In terms of breadth, SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and
              contains 100,788 samples of the quadruple (question, image, chosen response, rejected response). In terms
              of depth, the responses are collected from 12 open-source (e.g., QwenVL) and closed-source (e.g., Gemini)
              VLMs to ensure diversity.
              The experimental results indicate that models trained with alignment techniques on the SPA-VL dataset
              exhibit substantial improvements in harmlessness and helpfulness while maintaining core capabilities.
              SPA-VL, as a large-scale, high-quality, and diverse dataset, represents a significant milestone in
              ensuring that VLMs achieve both harmlessness and helpfulness.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Overview -->
  <section class="section">
    <div class="container is-max-desktop content">
      <h2 class="title">Overview</h2>
      <p><strong>Data Statistics.</strong> Our SPA-VL dataset comprises four parts: the training set, the validation
        set, and two test sets, HarmEval and HelpEval, which are used to evaluate harmfulness and helpfulness,
        respectively.
        The number of samples in each part is 93,258, 7,000, 265, and 265, respectively.
        Following table hows the dataset statistics of the training set.</a>
      <figure>
        <img src="static/images/3.png" alt="Figure 3: Dataset Statistics">
      </figure>
      <!-- Each image is accompanied with three questions of varying difficulty levels: easy questions, hard questions, and hard statements. -->
      To detect the unsafe content covered by our SPA-VL dataset, we utilize the MD-Judge evaluator to calculate the
      unsafe rate of the collected questions and VLMs' responses.</a>
      Nearly half of the collected questions are unsafe, while the unsafe rate for the chosen response and rejected
      response is 11.7% and 42.23%, respectively.
      The HarmEval test set includes a substantial number of harmful questions, while the HelpEval test set primarily
      comprises questions that involve instruction following or require the expression of opinions.
      </p>
      <p><strong>Diverse Domains.</strong>
        A diverse and representative set of images is essential for training models to handle vision data safely and
        effectively. Our primary challenge is ensuring diversity while maintaining relevance to harmful content
        categories. To address this, we establish a comprehensive harm content categorization framework.
        As shown in above figure, our SPA-VL adopts 6 primary domains, 15 secondary categories,
        and 53 tertiary categories, ensuring comprehensive coverage and granularity for precise harm detection and
        response alignment.</a>
      </p>
      <p><strong>Data Formats.</strong>
        We gather preference data by choosing the better response
        from two generated by VLMs, based on predefined criteria of harmlessness and helpfulness.</a>
        Finally, a quadruple <i>(question, image, chosen response, rejected response)</i> reflecting preferences is
        collected, where the chosen response is the better response selected under the principle of harmlessness and
        helpfulness.

      </p>
    </div>
  </section>
  <!-- End Overview -->

  <!-- Results -->
  <section class="section">
    <div class="container is-max-desktop content">
      <h2 class="title">Results</h2>
      <p><strong>Main Results.</strong> As shown in the first table, the models trained on our SPA-VL dataset,
        LLaVA-SPA-VL-DPO and LLaVA-SPA-VL-PPO, which are the best safety models from our training, exhibit superior
        safety performance. They surpass the baseline model LLAVA-1.5 (7B) and other open-source models, whether or not
        those models have undergone safety alignment.</a>Specifically, our models achieve best safe result on
        MM-SafetyBench, AdvBench and HarmEval tests. Notably, the LLAVA-HH-Harmless-PPO model, trained on the purely
        language-based Anthropic Harmless preference
        dataset, performs well in the AdvBench dataset and the text-only components of MM-SafetyBench. However, its
        performance drops significantly in safety tests involving images. This underscores the necessity of
        incorporating image data into safety alignment datasets for VLMs.</a> In addition to evaluating the safety
        performance, we also validate our models' general ability.
      <figure>
        <img src="static/images/4.png" alt="Figure 4: General Ability">
      </figure>

      </p>

      <p><strong>Data Scale.</strong> We delve into the impact of varying amounts of data on the performance of
        alignment models. Across different data quantities (around 100, 1k, 5k, 10k, 30k, and 90k), we
        conduct experiments encompassing various evaluation metrics.
      <figure>
        <img src="static/images/5.png" alt="Figure 5: Data Scale">
      </figure>

      </p>

      <p><strong>Response Model Selection.</strong> We examine the impact of response diversity and safety in our
        dataset on model training. We conducted four groups of experiments, each group is trained using DPO on around
        10K samples. Safe Group consists of response pairs from the three safest models (InternLMXComposer, QwenVL,
        Gemini1.0 Pro Vision).
        Relative Safe Group includes pairs from relative safe models(LAMM_SFT, LLaVA1.5, InternLMXComposer, QwenVL,
        gemini). Unsafe Group comprises pairs from the five least safe models(mPLUG-Owl, Otter, InstructBLIP,
        LLaMA-Adapter-v2, Gemini-Jailbreak) and the All group consists of the complete set of 12 models.

      <figure>
        <img src="static/images/6.png" alt="Figure 6: Response Model Selection.">
      </figure>

      </p>

      <p><strong>Question Types.</strong> We also analyze the impact of three different question types(Easy
        questions, Hard questions, and Hard statements) on the experimental results. We compare these individual
        results with the combined results of all three question types. For each experiment, we select training dataset
        of approximately 10k instances and using dpo to train our model.%and present the harmlessness validation
        results following the DPO experiments.
      <figure>
        <img src="static/images/7.png" alt="Figure 7: Question Types.">
      </figure>

      </p>
    </div>
  </section>
  <!-- End Results -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{zhang2024spavlcomprehensivesafetypreference,
        title={SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model}, 
        author={Yongting Zhang and Lu Chen and Guodong Zheng and Yifeng Gao and Rui Zheng and Jinlan Fu and Zhenfei Yin and Senjie Jin and Yu Qiao and Xuanjing Huang and Feng Zhao and Tao Gui and Jing Shao},
        year={2024},
        eprint={2406.12030},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2406.12030}, 
  }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>